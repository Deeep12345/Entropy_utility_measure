\documentclass[a4paper, twoside]{report}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{hyperref}
\hypersetup{colorlinks=false}
\usepackage{lscape}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage[colorinlistoftodos]{todonotes}

\usepackage{float}
\usepackage{colortbl}
\usepackage{subfigure}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{pseudocode}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{thm}{Theorem}
%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

\makeatletter
\newcommand*{\centerfloat}{%
  \parindent \z@
  \leftskip \z@ \@plus 1fil \@minus \textwidth
  \rightskip\leftskip
  \parfillskip \z@skip}
\makeatother


\title{The Limits of Information Loss Metrics as Measures for Utility}
\author{Romain de Spoelberch}

\begin{document}
\input{title/title.tex}

\begin{abstract}
$k$-Anonymity is a popular privacy framework for the anonymization of tabular data, where one record is indistinguishable from at least $k-1$ others. This is typically done through generalization and suppression, with many degrees of freedom left to analysts. 

This allotted freedom gives rise to an exponential number of ways to $k$-anonymize a dataset. As a method to compare the utility of these different $k$-anonymous versions, a range of heuristic metrics have been proposed. However, the extent to which these metrics capture useful features of the data is unclear.

In this project, we propose the first assessment of these information loss metrics' power to predict utility in real-world tasks.

We consider simple machine learning tasks on several standard datasets, which we anonymize with state-of-the-art algorithms, varying a range of anonymization hyper-parameters.

We find that most metrics in the literature have tenuous links with utility, and that, between two $k$-anonymous versions of a dataset, trying to predict the one with more utility based on any individual metric only yields better-than-random results by 15\%, on average.

Following that, we train a model on the metrics to predict utility. This Meta-Metric represents the best case use for metrics, combining their predictive power. We show the Meta-Metric only improves our results by an additional 10\%. 
These results suggest that existing utility metrics are bad predictors for the utility of real-world tasks and cannot, in practice, guarantee the selection of the best anonymization procedure.
\end{abstract}

\renewcommand{\abstractname}{Acknowledgements}
\begin{abstract}
The lion's share of thanks go to Florimond Houssiau. He proposed the project, and generously devoted time every week to offer great advice. More than anything, his help taught me to think more creatively and analytically, a skill I hope to carry through no matter where I end up.

I would also like to thank Yves-Alexandre de Montjoye, whose ability to understand the crux of a problem, ask the right questions, and come up with creative solutions helped push this project forward and into new directions multiple times.

Of course, I also wish to thank my family for their unrelenting and unquestioning support, no matter what I get myself into. Finally, thanks to the Entity for making my days in Huxley ones that I'll never forget. 
\end{abstract}

\tableofcontents
% \listoffigures
% \listoftables

\input{introduction/introduction.tex}
\input{background/background.tex}
\input{project/experiment.tex}
\input{project/results.tex}
\input{evaluation/evaluation.tex}
\input{conclusion/conclusion.tex}
\input{appendix/appendix.tex}

\bibliographystyle{unsrt}
\bibliography{bibs/sample}
\addcontentsline{toc}{chapter}{Bibliography}

\end{document}