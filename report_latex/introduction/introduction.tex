\chapter{Introduction}
In our increasingly digitalized world, every step of our day leaves a trace \cite{data_stats}. Per minute, Venmo-- a digital wallet that facilitates payments-- processes over $\$160,000$, each transaction containing information on both the sender and the recipient; Uber records over $9500$ rides, amassing information on users' movements, and AirBnB arranges $1500$ reservations. All this data can be used for good-- for example, in optimising services, or inferring how to better meet customers' needs.

Nevertheless, the collection of data poses privacy concerns. For example, recently, Google's project Nightingale has raised privacy red flags. The company, having struck a deal with one of America's largest healthcare provider, is getting the personal medical data of up to fifty million Americans \cite{guardian_data}. Their goals to further the field of medicine with data and artificial intelligence, laudable as they are, have left many suspicious. Some worry that the big tech company stands to gain a lot from using all of this information for alternative reasons, targeting individuals or selling the data, committing a major privacy breach in either case. 

Privacy concerns are taken seriously, as demonstrated by the 2016 European regulation: General Data Protection Regulation (GDPR). This regulation tightens control on data collection in Europe, and restricts the uses (processing) allowed \cite{gdpr_basic}. Along with a crackdown on what constitutes a privacy breach, GDPR includes Recital 26 \cite{recital26}, stating that the privacy measures instituted 
``should apply to any information concerning an identified or identifiable natural person''. Essentially, to use data like we used to, and to avoid the severe penalties put in place by the EU for privacy breaches, data must be anonymous. Where anonymity was important, ethically speaking, it is now mandatory.

Researchers have developed many anonymization methods to protect people's privacy, including one called $k$-anonymity \cite{kanon_orig}. $k$-Anonymization ensures that no individual in a dataset is distinguishable from $k-1$ others; it is often done by generalizing records -- removing detail from specific information so that individuals fit in the same broader categories. Additional privacy measures have to be put in place for the dataset to truly be considered anonymous but k-anonymization forms a good basis \cite{critique_kanon} and is widely used in the industry \cite{google2020anonymizing}.

There are exponentially many ways to $k$-anonymize a dataset, and each method entails the loss of varying amounts of information within the dataset. Analysts need datasets with as much preserved information as possible-- despite the coarsening of detail incurred-- to allow for accurate and diverse inferences. This is challenging and analysts cannot simply rely on their instincts. To solve this issue, researchers have presented a series of heuristic metrics that measure a proxy for the 
``utility'' of anonymized datasets \cite{ambiguity_metric, cm_granularity_metric, discern_metric, dse_metric, entropy_measure, hellinger&biv, ilm}.

These metrics have become pervasive in anonymization, and the attempt to maximize utility. For example, some $k$-anonymity algorithms are built with the aim of optimising for certain metrics like Sweeney's MinGen, optimising Precision \cite{kanon_algos}. Other algorithms, like Mondrian, are first presented with their results on existing information loss metrics to convince of their performance \cite{mondrian}. Anonymization toolboxes, like ARX, are available online. While they implement a hybrid version of multiple $k$-anonymization algorithms, they calculate a range of utility metrics for the anonymized dataset, leaving the metrics' importance up to the user \cite{arx}.

Although theses metrics are widely used, it is unclear whether they capture essential features of the data or if they can accurately judge a dataset's utility-- there is no evidence directly linking the metrics and utility. For that reason, in this project, we empirically test the correlation between the usefulness of a $k$-anonymous dataset, and its metric scores. We train off-the-shelf classifiers on the anonymous datasets, and equate these models' performances on real test data to utility. 

We create 600 anonymous versions of 4 datasets, heavily varied in terms of sizes and origins, using different hyperparameters (such as algorithm used, generalization trees, or the chosen $k$ value). 

We compare the models' performances to a wide range of utility metrics from the literature. Our results show that, taken individually, metrics are not good predictors for utility and any trend that can be observed is specific to the combination of dataset anonymized, algorithm used, and classifier used, implying very little applicability as general utility measures. We further tested whether the individual metrics could be used to choose the most useful of two $k$-anonymous versions of the same dataset. This is a situation that, currently, analysts will often rely on metrics for. The results show most metrics rarely outperform a random choice by more than 15\%. This suggests that the utility metrics commonly used to generate and evaluate anonymized datasets might not accurately predict the performances of simple machine learning tasks.


Finally, we propose an experiment to show that even in the best case, when an optimal combination of all our metrics is used, the metrics still do not have as much predictive power as implied in the literature:  we train a model to predict the accuracy of the classification task using the metrics, in a sense combining these metrics in a ``Meta''-Metric with as much predictive power as possible for a specific classification task. 

At their most predictive, the combination of metrics only improves our result by an additional 10\%. Additionally, we show these Meta-Metrics are task-specific; they are incapable of transferring their results to different datasets, different $k$-anonymization algorithms, or different classifiers. This means that ``expertise'' for a task is not transferable, and confirms that metrics are unusable as general utility measures.
